# ReactFace: Multiple Appropriate Facial Reaction Generation in Dyadic Interactions

## Under construction. You can also refer to our baseline in [REACT 2023 Multimodal Challenge](https://github.com/reactmultimodalchallenge/baseline_react2023) first.

[[Homepage]](xxxxxxxx)  [[Reference Paper]](https://arxiv.org/pdf/2305.15748) [[Code]](https://github.com/lingjivoo/ReactFace)



## ğŸ› ï¸ Installation

### Basic requirements

- Python 3.8+ 
- PyTorch 1.9+
- CUDA 11.1+

### Install Python dependencies (all included in 'requirements.txt')

```shell
conda create -n react python=3.8
conda activate react
pip install git+https://github.com/facebookresearch/pytorch3d.git
pip install torch==1.9.1+cu111 torchvision==0.10.1+cu111 torchaudio==0.9.1 -f https://download.pytorch.org/whl/torch_stable.html
pip install -r requirements.txt
```

## ğŸ‘¨â€ğŸ« Get Started 
<details><summary> <b> Data Download</b> </summary>
<p>

- The REACT 2023 Multimodal Challenge Dataset is a compilation of recordings from the following three publicly available datasets for studying dyadic interactions: [NOXI](https://dl.acm.org/doi/10.1145/3136755.3136780), and [RECOLA](https://ieeexplore.ieee.org/document/6553805).

- Participants can apply for the data at our [Homepage](https://sites.google.com/cam.ac.uk/react2023/home).


**Data organization (`data/`) is listed below:**
```data/partition/modality/site/chat_index/person_index/clip_index/actual_data_files```
The example of data structure.
```
data
â”œâ”€â”€ test
â”œâ”€â”€ val
â”œâ”€â”€ train
   â”œâ”€â”€ Video_files
       â”œâ”€â”€ NoXI
           â”œâ”€â”€ 010_2016-03-25_Paris
               â”œâ”€â”€ Expert_video
               â”œâ”€â”€ Novice_video
                   â”œâ”€â”€ 1
                       â”œâ”€â”€ 1.png
                       â”œâ”€â”€ ....
                       â”œâ”€â”€ 751.png
                   â”œâ”€â”€ ....
           â”œâ”€â”€ ....
       â”œâ”€â”€ RECOLA
       â”œâ”€â”€ UDIVA
   â”œâ”€â”€ Audio_files
       â”œâ”€â”€ NoXI
       â”œâ”€â”€ RECOLA
           â”œâ”€â”€ group-1
               â”œâ”€â”€ P25 
               â”œâ”€â”€ P26
                   â”œâ”€â”€ 1.wav
                   â”œâ”€â”€ ....
           â”œâ”€â”€ group-2
           â”œâ”€â”€ group-3
       â”œâ”€â”€ UDIVA
   â”œâ”€â”€ Emotion
       â”œâ”€â”€ NoXI
       â”œâ”€â”€ RECOLA
           â”œâ”€â”€ group-1
               â”œâ”€â”€ P25 
               â”œâ”€â”€ P26
                   â”œâ”€â”€ 1.csv
                   â”œâ”€â”€ ....
           â”œâ”€â”€ group-2
           â”œâ”€â”€ group-3
       â”œâ”€â”€ UDIVA
   â”œâ”€â”€ 3D_FV_files
       â”œâ”€â”€ NoXI
       â”œâ”€â”€ RECOLA
           â”œâ”€â”€ group-1
               â”œâ”€â”€ P25 
               â”œâ”€â”€ P26
                   â”œâ”€â”€ 1.npy
                   â”œâ”€â”€ ....
           â”œâ”€â”€ group-2
           â”œâ”€â”€ group-3
       â”œâ”€â”€ UDIVA
            
```
 
- The task is to predict one role's reaction ('Expert' or 'Novice',  'P25' or 'P26'....) to the other ('Novice' or 'Expert',  'P26' or 'P25'....).
- 3D_FV_files involve extracted 3DMM coefficients (including expression (52 dim), angle (3 dim) and translation (3 dim) coefficients.
- The frame rate of processed videos in each site is 25 (fps = 25), height = 256, width = 256. And each video clip has 751 frames (about 30s), The samping rate of audio files is 44100. 
- The csv files for baseline training and validation dataloader are now avaliable at 'data/train.csv' and 'data/val.csv'
 
 
</p>
</details>



<details><summary> <b> External Tool Preparation </b> </summary>
<p>

We use 3DMM coefficients to represent a 3D listener or speaker, and for further 3D-to-2D frame rendering. 
 
The baselines leverage [3DMM model](https://github.com/LizhenWangT/FaceVerse) to extract 3DMM coefficients, and render 3D facial reactions.  

- You should first download 3DMM (FaceVerse version 2 model) at this [page](https://github.com/LizhenWangT/FaceVerse) 
 
  and then put it in the folder (`external/FaceVerse/data/`).
 
  We provide our extracted 3DMM coefficients (which are used for our baseline visualisation) at [Google Drive] (https://drive.google.com/drive/folders/1RrTytDkkq520qUUAjTuNdmS6tCHQnqFu). 

  We also provide the mean_face, std_face and reference_full of 3DMM coefficients at [Google Drive](https://drive.google.com/drive/folders/1uVOOJzY3p2XjDESwH4FCjGO8epO7miK4). Please put them in the folder (`external/FaceVerse/`).

 
Then, we use a 3D-to-2D tool [PIRender](https://github.com/RenYurui/PIRender) to render final 2D facial reaction frames.
 
- We re-trained the PIRender, and the well-trained model is provided at the [checkpoint](https://drive.google.com/drive/folders/1Ys1u0jxVBxrmQZrcrQbm8tagOPNxrTUA). Please put it in the folder (`external/PIRender/`).
   
</p>
</details>
